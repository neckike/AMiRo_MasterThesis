%IMPORTANT!!!!!! remember to abilitate twoside, openright to print document.
%remember the clearpages for editing better.

\documentclass[12pt]{report}%, twoside, openright]{report}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\graphicspath{{diagrams/}}
\usepackage{listings}
\usepackage{dirtytalk}

\usepackage[backend=biber,
						bibstyle=ieee,
						sorting = nty,
						citestyle=numeric-verb]{biblatex}
\bibliography{bibliography.bib}
\begin{document}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

\center % Center everything on the page

\textsc{\LARGE Fachhochschule Dortmund}\\[1.5cm] % Name of your university/college
\textsc{\Large Master Thesis for M. Eng. in Embedded Systems for Mechatronics}\\[0.5cm] % Major heading such as course name

\HRule \\[0.4cm]
{ \bfseries Development of a real-time software architecture for AMiRo robot based on the operator controller-module}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
Hector Gerardo Munoz Hernandez
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisors:} \\
Prof. Dr. Carsten Wolff

Uwe Jahn
\end{flushright}
\end{minipage}\\[2cm]

{\large \today}\\[2cm]

\vfill
\end{titlepage}
\pagenumbering{roman}
\tableofcontents
\listoffigures
\listoftables

\begin{abstract}
This work covers most of the internal controller required functionality assigned to the DA\_AMiRo project. The architecture of the AMiRo is briefly discussed and it is then explained how the three boards of the AMiRo talk to each other. Some other sensors and communication protocols are explained like the Communication Area Network module. All these functionalities are  being administrated in the sensor data handler which is explained later. Every piece of code described in the present report is part of the DA\_AMiRo project and the reader can go to the gitlab repository and the Wiki page for more information. One of the purposes of the DA\_AMiRo project is to compare its functionality with the DAEbot project conducted by Uwe Jahn. The reader can read all about the DAEbot's internal controller in the previous student's work \cite{DAEBot_internal}. This work will finalize with a comparison on both systems.
\end{abstract}
\pagenumbering{arabic}

\chapter{Introduction}
\section{Real-time Operating Systems}
The necessity to meet deadlines in today's projects is every time more demanded. Most applications consist of different tasks that can be executed in parallel, have different priority, have different period repetition, and frequently use shared resources. Using a multi-threading approach, however,  can create a challenge in terms of performance, stability and time of implementation.

An operating system is a software component of a computer system that is responsible for the management and coordination of activities and the sharing of the resources of the computer \cite{mcgraw}. "A real-time operating system perform these tasks, but is designed to run applications with a very precise timing and a high degree of reliability. This can be especially important in measurement and automation systems where downtime is costly or a program delay could cause a safety hazard" \cite{rtos}.

In other words, a real-time operating system is an operational system that provides certain capability according to preset timing constraints \cite{whatisRTOS}. This is achieved through a scheduler that is designed to provide a deterministic execution pattern. And so, the RTOS are used in systems with critical timing requirements, when each task should be executed within a restricted time interval \cite{whatisRTOS}.

When it can be guaranteed that a task will never exceed a maximum amount of time in being completed, it can be said that the operative system is "hard real-time". If it can be guaranteed that a task will most of the time not exceed a maximum amount of time in being completed, it can be said that the operative system is "soft real-time". An airbag system is an example of a "hard real-time" system, where the task has to be guaranteed to execute always within a time limit. For a "soft real-time" example, a video streaming can be considered, where an occasional loss of data can be accepted because it does not compromise the whole functionality \cite{rtos}.

In the real-time operating systems the managing of the tasks is crucial. To this end, programmers have to decide which task is running at certain time and how other tasks can preempt the processing resources if their priorities are higher. In other words, manage the schedule of the tasks so that they meet with their deadlines.

Some of the main characteristics of a RTOS are the following:

\begin{itemize}
\item Determinism: An application timing can be guaranteed within a certain margin of error.
\item Soft and Hard Real-Time: The validity of data after meeting a deadline. In soft real-time the deadline is not that crucial but in hard real-time the meeting of the deadline is very important.
\item Jitter: Which is the amount of error in the timing of a task over subsequent iterations of a program or loop \cite{whatisRTOS}.
\end{itemize}

\section{Operator Controller Module}

The DA\_AMiRo Operating System is inteded to be compared to the OCM \cite{ocmAuto} software architecture approach the DAEbot is using. In this structure there are three basic controllers: Internal Controller, Reflective Operator, and Cognitive Operator. In order to have a more precise overview of how these three controllers work in the DAEbot, figure \ref{fig:ocm} is explained by a direct quote from the Wiki:

\say{Refering to figure \ref{fig:ocm}, the Controllers (red) are used to connect all sensors and actuators. The Controllers need to be configured and programmed once, so the software on the controllers is mostly fixed and does not need to be flexible. The Reflective Operator(s) (yellow) control the Controllers. The Reflective Operator's software is the adaptable part of the distributed system - it changes from application to application. These Operator Controller Module(s) are located on the robot itself and let it act autonomously. The Cognitive Operator is hosted on a server and is used to optimize, program and interact with the user. The Cognitive Operator is connected via a wireless connection and can (only) interact with the Reflective Operator(s).}\cite{DAEbot_Wiki}.

\begin{figure}[h!t]
	\centering
	\includegraphics[width=\textwidth]{ocm}
    \caption{OCM software arquitecture for the DAEbot\cite{DAEbot_Wiki}.}
    \label{fig:ocm}
\end{figure}
\clearpage

The OCM was introduced by a group of Professors in the University of Paderborn. The concept aimed to structure and design a reconfigurable controller systems \cite{ocmAuto}. As mentioned before, there are three different levels in the OCM. The first one is the lowest level of the OCM also simply known as the Controller. This level interacts directly with the plant of the system, which means that the control signal is produced and measured here. It is also necessary that the software processing in this layer is quasi-continuous so that the measured values are processed in real-time conditions \cite{ocmAuto}.

The second layer is called the reflective operator. It is in this layer where the monitoring and the controlling routines are executed \cite{ocmAuto}. This layer does not have access to the actuators directly, because the Controller is already doing this. The expected result is that the Reflective Operator modifies the Controller, sometimes also changing between different pre-established Controller configurations. This level also requires a quasi-continuous operations like a continuous adaptation algorithm or a watchdog. This operator also has to operate under hard-time constraints because its relation with the Controller. In short, this layer serves as a interface between the Cognitive Operator and the Controller \cite{ocmAuto}.

The third layer is the Cognitive Operator. On this layer the system gathers information about itself and its environment to improve itself. This recollection of information can be done by applying various methods such as learning and model-based optimization among others. In other words, it is in this layer where the self-improvement takes place \cite{ocmAuto}.

This project compares the Controller of the DAEbot with the Controller of the DA\_AMiRo, and Reflective Operator of DAEbot to the Cognition board of DA\_AMiRo. The controller is the responsible for commanding the internal sensors of the STM32F32 boards inside DA\_AMiRo in real-time. This sensor information is then retrieved by the Cognition board with a demo application that will be explained later in this document inside section \ref{sect:example}.

\section{AMiRo}
The AMiRO project, which stand for Autonomous Mini Robot was started in the University of Bielefeld by Stefan Herbrechtsmeier and worked by Thomas Schöpping in collaboration with few others. The motivation for this project was to use small embedded systems in order to create a small robot that is easy to transport, usable on a table or the floor, appealing for young people, that is created with a modular approach is able to be extendable and customizable. With all of these aspects in mind, the result should be also a powerful tool for research and education \cite{AMiRo_ppt_v1}.

The preliminary work was conducted in the University of Paderborn in the form of the BeBot mini robot \cite{AMiRo_ppt_v1}. The intended architecture of both robots is a modular one, which means that it has more than one board functioning as a unity. In AMiRo's case, said architecture consisted on three boards, DiWheelDrive, Power Management, and Light Ring that will be discussed in the next chapter, and two main extension boards: the Cognition board and the Image Processing board. The former will be explained in section \ref{sub:cogn}, but the latter will not be discussed in this work, as it was not needed for the current reach of the project. If the reader is interested in the Image Processing board, he or she can refer to the project presentation given by Prof. Herbrechtsmeier \cite{AMiRo_ppt_v2}.

Physically AMiRo has a cylindrical shape with a diameter of 100mm. It is covered by a chassis that does not cover the DiWheelDrive board's bottom as this board has the proximity sensors pointing towards the ground. There are also two wheels with two separate motors directly connected to the DiWheelDrive board and two metal sliders for the robot stabilization in flat surfaces \cite{AMiRo_paper_modular}.

The chassis has three more openings for the following connections: USB serial that goes directly into the DiWheelDrive board, the power supply, and the charging pins. Additionally, when the Cognition board is attached, there are three more possibilities to physically interact with the robot: USB serial, USB dongle for wireless connection with the Linux OS, and a micro USB cable for connecting also with the Linux OS which is used in this board. The Cognition board will be later discussed in section \ref{sub:cogn}.

These characteristics make AMiRo a very compact and portable robot. Every board is designed to be modifiable, stackable and exchangeable with custom elements. All boards communicate with each other with the Controller Area Network protocol \cite{AMiRo_paper_modular}. This way every board can read any message from the CAN bus or write into it every time it is necessary.

\chapter{Architecture of AMiRo}
The AMiRo project was started in 2015 and ever since, the repository supporting the AMiRo project has had several updates \cite{AMiRo_Wiki}. It is worth mentioning that the version used as the basis for the present work was the version 1.0 stable. The AMiRo's repository consists of three basic folders: AMiRo bootloader, AMiRo Operating System, and ChibiOS.

Although this software architecture will be explained with more detail in section \ref{sec:soft}, it is worth mentioning that AMiRo's bootloader was used as it is in version 1.0 from the original repository in order to configure the computer for programming the AMiRo. The ChibiOS was also used as it is in version 1.0 from the repository \cite{AMiRo_Wiki}. The AMiRo Operating System was hugely modified to meet the requirements of the present work, which can be found under the current project's repository \textit{DA\_AMiRo} \cite{AMiRo_Git}.

In figure \ref{fig:real_1} the AMiRo robot with its three basic boards is presented with and without chassis. In the picture with chassis, a hand is presented to help visualize the actual size of the AMiRo, and in the photograph without chassis some of the components of the AMiRo can be seen. In order from bottom to top, the three basic boards are: \textit{DiWheelDrive}, \textit{PowerManagement}, and \textit{LightRing}. It is also possible to see the wheels, the motors, and the batteries attached to their corresponding boards. These structure will be explained in the next section.
%\footnote{AMiRo's original repository: https://openresearch.cit-ec.de/projects/amiro-os/wiki \cite{AMiRo_Wiki}}.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{real_amiro_1}
    \caption{AMiRo with and without chassis \cite{AMiRo_paper_modular}.}
    \label{fig:real_1}
\end{figure}
\clearpage

\section{Hardware Architecture of AMiRo}
As mentioned in the previous chapter, the AMiRo is equipped with three basic board modules and two extension boards. In this section each board will be briefly discussed with a special emphasis in the functionalities that are of interest for this work. For a more complete discussion about the boards or the AMiRo in general, the reader is invited to read the documentation referenced in this work, specially \cite{AMiRo_paper_verstaile, AMiRo_paper_modular, AMiRo_paper_applications, AMiRo_ppt_v1, AMiRo_ppt_v2} which can be found on the CITEC website or on the repository of this work on gitlab \cite{AMiRo_Git}.

The functionality of the sensors used in the project will be briefly explained in this chapter. For the explanation on how to read the output of the sensors that are sent via the Controller Area Network protocol, the reader is invited to jump directly into section \ref{sec:canconv} for reviewing the CAN convention used in the entire project and section \ref{sec:sensorout} for understanding each sensor's output.

In figure \ref{fig:schemat} a schematic of the AMiRo can be seen, with the three basic boards \textit{DiWheelDrive}, \textit{PowerManagement}, and \textit{LightRing} but also the extension boards \textit{ImageSensor}, \textit{Cognition}, and \textit{Image Processing}. The electrical interfaces are also displayed. For the current work, the most important thing to note in this image is that every board is able to talk to the CAN bus.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{amiro_schematic}
    \caption{Inner parts of AMiRo with all extension boards, and overview of the electrical interfaces \cite{AMiRo_paper_modular}.}
    \label{fig:schemat}
\end{figure}
\clearpage

\subsection{DiWheelDrive}
\label{sub:DWD}
The DiWheelDrive is the bottom board of the AMiRo. It is equipped with an ARM Cortex-M3 based STM32F103 MCU from STMicroelectronics \cite{AMiRo_paper_modular}. It is through the DiWheelDrive's UART interface that the AMiRo's three basic modules get programmed. Each board gets programmed at a time with help of the AMiRo bootloader which will be discussed in section \ref{sub:btl}.

This board has two motors of 1W DC. The purpose of having two motors was to be able to have a differential kinematic to allow flexible movements \cite{AMiRo_paper_modular}. Among other sensors and actuators, the ones that are in this board and are being used in this work are: three axis gyroscope, accelerometer, and magnetometer, floor proximity sensors, the two motors, and optical motor encoders. The functionality and some specifications of the above mentioned sensors and actuators will be discussed in the next subsections.

As mentioned before, the DiWheelDrive board has a Cortex-M3 MCU for calculating such things as motion control, odometry, and dead reckoning algorithms at high sampling rates \cite{AMiRo_paper_modular}. This board can be accessed by the other boards via the Controller Area Network protocol or via USB UART interface directly with the user. It is important to mention that every board has also a USB UART interface to communicate directly with the user but they are covered by the chassis except for the DiWheelDrive board \cite{AMiRo_paper_modular}.

The \textit{DiWheelDrive} board ready to be mounted into the robot can be seen in figure \ref{fig:diwheel}. In this figure the board can be seen from both, the top and the bottom perspective.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{DiWheel_real}
    \caption{Top and bottom view of the DiWheelDrive board \cite{AMiRo_ppt_v1}.}
    \label{fig:diwheel}
\end{figure}
\clearpage

\subsubsection{Gyroscope}
The gyroscope is a sensor that measures rotational motion. The units of the resulting measurement are revolutions per second or degrees per second. As a quick example, in a balancing robot a gyroscope can be used to measure how much the robot has rotated from the desired position and this information can be sent to an actuator to make corrections in order to reach the desired angle \cite{gyrostheory}. A gyroscope normally consists of a mass that moves with constant angular momentum, so when the gyroscope is tilted the axis of rotation of said mass reacts to the applied rotational movements making the axis of rotation tilt. This tilting leads to a displacement of the capacitance fingers inside the sensor which changes the capacitance. Finally when compared to a reference capacitance level, the angular velocity can be determined \cite{AMiRo_ppt_v1}.

The gyroscope used in the DiWheelDrive board is the l3g4200d from STMicroelectronics. This sensor communicates via SPI with the processor. This sensor is configured to have a resolution of $+/-500 dps$ in accordance with the user's manual \cite{gyroscopepart} and it is possible to obtain the output in degrees per second, micro degrees per second, revolutions per second, and micro revolutions per second with the actual code. The actual units are degrees per second but the user can easily change the output selecting a different function from \textit{updateSensorVal()} function inside DiWheelDrive.cpp. For a more clear explanation on the code structure, see section \ref{AMiRo_OS}. The sensor communicates with the processor via SPI communication.

\subsubsection{Accelerometer}
The accelerometer is a sensor that measures the acceleration of an object. The units of the resulting measurement are meters per squared second or in g forces, where g as a unit equals $9.81 m/s^2$ \cite{accelerometertheory}. It is typically a system composed by a mass, a spring, and a damper. The mass moves according to the applied acceleration and changes the resulting capacitance that can be sent as an output for further reading and processing \cite{AMiRo_ppt_v1}.

The accelerometer used in the DiWheelDrive board is the lis331dlh from STMicroelectronics. This sensor is configured be scaled with a $+/-8g$ factor in accordance with the user's manual \cite{accelerometerpart}. The output is in g units and the sensor is being communicated via SPI with the processor.

\subsubsection{Magnetometer}
The magnetometer is a sensor that measures the magnetic field for all three physical axes \cite{magnetometertheory}. This sensor works thanks to three principles. The first one is called the Anisotropic magneto resistance effect discovered in 1857 by William Thomson, in which an external magnetic field can make the sensor change the electrical resistance of a ferromagnetic material. The electrical resistance will be at a maximum value when the direction of the electrical current is parallel to said magnetic field \cite{AMiRo_ppt_v1}.

The second principle is called Fluxgate and it is a way of calculate the vectorial measurement of a magnetic field invented by Friedrich Förster in 1937. The method starts with two cores wrapped by two coils of wire. There should be an electrical current driving the core through cycles of magnetic saturation. The resulting electrical current of the second coil will depend on the external magnetic field \cite{AMiRo_ppt_v1}.

The last principle is called the Hall effect discovered in 1897 by Edwin Hall. In this principle can be visible when an electric current flows through a conductor in a magnetic field, the magnetic field will exert a transverse force on the moving charge carriers which tends to push them to one side of the conductor. The result is a voltage difference between both sides of the conductor \cite{halleffect}.

The magnetometer used in the DiWheelDrive board is the hmc5883l from STMicroelectronics. This sensor is configured to have a field resolution of $+/-5Gauss$ according to the user's manual \cite{magnetometerpart} and the current output is set to be in Gauss units. The sensor is being communicated via I2C protocol with the processor.

\subsubsection{Floor Proximity Sensor}
\label{sec:AMS}
The floor proximity sensor used is the "Fully Integrated Proximity and Ambient Light Sensor With Infrared Emitter", part vcnl4020 of Vishay \cite{proxsensor}. Four of these sensors are in the bottom of the AMiRo, programmed as light sensors, while other eight of these sensors are being used as proximity sensors and ambient light detectors coordinated by the Power Management board, see section \ref{PWB} for more information.

The ambient light sensor measures the intensity of light and its is measured in luminance. In order to do this the sensor usually consists of a photoresistor or a photo-sensitive material, outputting a resistance that can be measured and compared to a default value to estimate the luminance \cite{amstheory}.

The goal of providing AMiRo with these sensors attached to its bottom is to detect changes in the color of the floor. An example application can be for the AMiRo to follow a line or path painted on the ground, by making the robot recognize when it is over one color and when it is not.

\subsubsection{Motors}
As previously described in section \ref{sub:DWD}, the AMiRo has two DC motors of 1W each to offer a differential kinematic and to allow agile movements \cite{AMiRo_paper_modular}. These motors are connected to the DiWheelDrive board which means that this board controls the velocity of the motors and also knows the actual velocity of the motors.

The velocity of the motors can be set anytime from the Cognition board and the actual velocity of the AMiRo is one of the sensor values that can be asked from AMiRo's internal controller. The user can set and view the velocity in $\mu m/s$ or in $\mu rad/s$.

\subsubsection{Encoder}
AMiRo has optical encoders as well, that allow the user to know how much the motors have turned from a certain checkpoint. This data can be used to know how far the AMiRo has moved from a certain point. This sensor returns the $x$ coordinate in $\mu m$, the $y$ coordinate in $\mu m$, and the orientation of the wheels in $\mu rad$.

\subsection{Power Management}
\label{PWB}
The Power Management board is the central basic module of AMiRo. This board is responsible for the power supply and elementary behavior of the system. This is why it has the most powerful microcontroller. This board has an ARM Cortex-M4 based STM32F405 from STMicroelectronics. The characteristics from this microcontroller were needed to ensure a fast reaction times on critical system events and still provide enough headroom to additionally perform basic behaviors like obstacle avoidance, or homing \cite{AMiRo_paper_modular}.

There are two lithium-ion batteries connected to the Power Management board that power the whole system, and there are also some sensors on this board. Among the sensors that this board has, the ones that are being used by this project are: eight proximity sensors that act as proximity sensors and also as ambient light sensors, sensors for knowing if the charging cable is connected, the remaining battery charge or the time for a complete battery charge, the time for this both events to be completed, and the actual current consumption  \cite{AMiRo_paper_modular}.

The \textit{PowerManagement} board ready to be mounted into the robot can be seen in figure \ref{fig:powerm}. In this figure the board can be seen from both, the top and the bottom perspective.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{power_real}
    \caption{Top and bottom view of the Power Management board \cite{AMiRo_ppt_v1}.}
    \label{fig:powerm}
\end{figure}
\clearpage

\subsubsection{Power Status}
For the tracking of the power status, AMiRo is equipped with a bq27500 Impedance Track from Texas Instruments that allows the robot to obtain information such as remaining battery capacity, state-of-charge, run-time to empty, battery voltage, and temperature \cite{impedancepart}.

\subsubsection{Proximity Ring}
As seen in section \ref{sec:AMS}, the eight vcnl4020 sensors in connected to the Power Management board are functioning as ambient light sensors and also as proximity sensors. For this last functionality, the sensor has a built-in infrared emitter and photo-pin-diode, having a maximum relying measurement of 200mm \cite{proxsensor}. These sensors are connected in a ring-fashion way so that the AMiRo has eight sensors throughout its entire circumference, making it able to detect obstacles in all horizontal directions. The units of the output are also in mm.

The \textit{Proximity Ring} ready to be mounted into the robot can be seen in figure \ref{fig:proxring}. The proximity ring attaches to the \textit{Power Management} board as mentioned before.

\begin{figure}[ht]
	\centering
	\includegraphics[]{prox_ring_real}
    \caption{Proximity Ring view \cite{AMiRo_ppt_v1}.}
    \label{fig:proxring}
\end{figure}
\clearpage

\subsection{Light Ring}
The Light Ring board is the top-most board in the AMiRo. This board has a STMicroelectronics ARM Cortex-M3 based STM32F103 MCU. It contains eight RGB LEDs arranged in a circular way corresponding to the Proximity Ring discussed in last section. These LEDs can be set from the Cognition board, setting each color of the RGB channels \cite{AMiRo_paper_modular}.

The \textit{LightRing} board ready to be mounted into the robot can be seen in figure \ref{fig:lightr}. In this figure the board can be seen from both, the top and the bottom perspective. As it can be seen in the image, the LEDs are positioned equidistant in a circular fashion. This board is the upmost board which is covered by a translucent material, allowing the user to see the colors of the LEDs.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{light_real}
    \caption{Top and bottom view of the Light Ring board \cite{AMiRo_ppt_v1}.}
    \label{fig:lightr}
\end{figure}
\clearpage

\subsection{Cognition Board}
\label{sub:cogn}
This board was thought as an extension board which is no longer from the basic set of boards that make the AMiRo. The Cognition board in this project is being compared with the Reflective Operator of the DAEbot project according to the OCM model. It provides all of the high-level tasks, gathering and coordinating the information generated in the three basic boards also known as the internal controller \cite{AMiRo_paper_modular}.

The most important feature of this board is the attached Gumstix Overo Computer-on-Module. In the current setup, the Overo TidalSTORM COM is being used, which combines an ARM Cortex-A8 based DaVinci DM3730 SoC from Texas Instruments \cite{AMiRo_paper_modular}. This board has the same serial programming port as on the three basic boards. It also has a Type A, a micro-AB, and an internal pin header USB interfaces \cite{AMiRo_paper_modular}.

The operating system running on the Overo COM is based on the Linux Yocto reference distribution Poky \cite{poky}, and the way it communicates with the three basic boards is through CAN, using a simple SocketCAN software-wrapper that is offered by this distribution \cite{AMiRo_paper_modular}.

The \textit{Cognition} board ready to be mounted into the robot can be seen in figure \ref{fig:cogb}. In this figure the board can be seen from both, the top and the bottom perspective.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{cognition_real}
    \caption{Top and bottom view of the Cognition board \cite{AMiRo_ppt_v2}.}
    \label{fig:cogb}
\end{figure}
\clearpage

\section{Software Architecture of AMiRo}
\label{sec:soft}
As it has been discussed in previous sections, the AMiRo project was started back in 2015 and it is still being maintained. The version 1.0 of the repository was the base used for this project \cite{AMiRo_Git}.

\subsection{AMiRo bootloader}
\label{sub:btl}
The open-source bootloader for MCUs is the bootstrap for the three basic boards of the AMiRo. The AMiRo's bootloader is the responsible of configuring and setting up the computer for working with the AMiRo. The set-up that is provided in the bootloader is more than enough to successfully start working on the Operating System of AMiRo and programming it \cite{btl}.

As said before, the DiWheelDrive is the only module of which the programming port can be accessed by the user. This is why new software is applied to the two other basic boards by remote flashing via OpenBLT. Using CAN communication the new binary is forwarded by the DiWheelDrive board so that the target board receives the data and stores it in its flash memory \cite{AMiRo_paper_modular}.

\subsection{AMiRo real-time operating system kernel}
For the real-time operating system the open-source ChibiOS was chosen. This project aims at creating a portable, light weight, and fast real-time operating system for embedded applications \cite{chibioshp}. ChibiOS not only provides low-level drivers for all interfaces of the most common microcontrollers, it also gives an unified hardware abstraction layer, priority based preemptive scheduling, and high-performance event and message passing systems. ChibiOS can be configured statically and dynamically to exactly fit the application in order to maximize efficiency and performance \cite{AMiRo_paper_modular}.

\subsubsection{ChibiOS}
Some of the main characteristics from ChibiOS are:

\begin{itemize}
\item System time: ChibiOS has a 16 or 32 bits system time counter.
\item Real tick-less mode: There is no periodic system tick for optimal power management.
\item IRQ Management: ISRs abstraction.
\item Preemption: Fully preemptive scheduling.
\item Round robin scheduling: Round robin scheduling for threads at equal priority.
\item Messages: Inter-thread synchronous messages.
\item Mailboxes: Message queues.
\item Counter semaphores: Semaphores with boolean state.
\item Binary semaphores: Semaphores with boolean state.
\item Mutexes: Mutexes implementing the priority inheritance algorithm.
\item Events: Events, event flags, event sources.
\item Dynamic services: Dynamic threading.
\end{itemize}
Note: All the above mentioned characteristics can be found and further explained in the ChibiOS homepage. \cite{chibioshp}

A project containing ChibiOS consists on the Operating System source files, few configuration files and a HAL file for the peripherals handling.

\subsection{AMiRo Operating System}
\label{AMiRo_OS}
AMiRo's Operating System or board specific abstraction layer from the original repository \cite{AMiRo_Wiki} includes two basic applications, the first one is a rudimentary command shell and the second one a hardware test. The first one is already integrated in ChibiOS and it is extended by module specific commands \cite{AMiRo_paper_modular}.

The structure of the AMiRo Operating System, including the files that are being used with the current project can be visualized in figure \ref{fig:OS}. The entire project is written in C++ language, with some inclusions of C code. In this figure, it can be seen that inside \textit{AMiRo\_OS} there is the \textit{components} folder which contains the shared codes that the three boards are using. The other part of the structure worth mentioning now is the \textit{devices} folder in which the three boards have their own set of files, always maintaining the same structure among them.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{AMiRo_OS}
    \caption{AMiRo\_OS breakdown structure}
    \label{fig:OS}
\end{figure}

%Quizás algo más a cerca de AMiRo (apps, applicaciones pensadas, demos....)

%\clearpage
\subsection{DA\_AMiRo software structure}
From the past sections the hardware and software structure of the AMiRo was mentioned and briefly discussed. From this point forward the structure and functionality of the DA\_AMiRo project will be discussed as it is the central point of the master thesis. As a friendly reminder, the reader can obtain the source codes made for the DA\_AMiRo project from the repository \cite{AMiRo_Git}.

In last section, the AMiRo's Operating System structure was introduced. This Operating System is the one located on top of figure \ref{fig:DAAMIROstr}, inside the folder called \textit{amiro\_1.0\_stable}. This Operating System was hugely modified to fit the requirements of the DA\_AMiRo project, and it's functionality will be discussed in section \ref{sub:DAAMIROOS}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{DAAMIRO}
    \caption{DA\_AMiRo overall structure}
    \label{fig:DAAMIROstr}
\end{figure}

The configuration files for the three projects are located inside the \textit{Devices} folder which can be visualized in figure \ref{fig:DAAMIROstr}. They configure the QT IDE for each of the three boards, so the user can easily code and create the binary to program the AMiRo's boards. It is also inside this folder where the \textit{Reflective Operator} is located. The Reflective Operator consists of a simple C code for programming the MuRoX Cognition board, but also has a Matlab project responsible for creating an AMiRo's compatible program from a Simulink model. These issues will be discussed further in chapter \ref{chap:reflective}.

Inside the \textit{Peripheral-Drivers} the user can find the ControllerAreaNetwork.h responsible of having the structures and all the definitions that are being used by both the Internal Controller corresponding to the three basic boards and the \textit{Reflective Operator} which is in charge of the MuRoX board. It is also inside this folder that the \textit{sensorhandler.cpp} and \textit{sensorhandler.h} are found. These both files manage the scheduling of the sensor data and they will be discussed in chapter \ref{chap:sensorDH}.

Finally, the folder \textit{Documentation} has some of the papers and information that the AMiRo has had since it was created. The reader can go inside this folder for getting more information about the AMiRo. In the other hand, the folder \textit{PusleAT} was created for a future reference and it is still a work in progress.

\subsection{DA\_AMiRo Operating System's functionality}
\label{sub:DAAMIROOS}
In this section, the functionality of the DA\_AMiRo Operating System from figure \ref{fig:OS} will be addressed.

The DiWheelDrive board, the DiWheelDrive class inherits from both ControllerAreaNetworkRx.cpp and from ControllerAreaNetworkTx.cpp, making it capable of sending and receiving CAN frames. This happens exactly the same in the three boards. The CAN frame structure will be further discussed in chapter \ref{chap:CAN}. The DiWheelDrive class also implements the functions for updating each of the sensor's values and sending independently the data via CAN interface. As it can be seen in figures \ref{fig:DWDclass}, \ref{fig:PMclass}, and \ref{fig:LRclass}, DiWheelDrive has virtual functions of every sensor including the Power Management board and also the Light Ring board. These functions however, have a '= 0' which means that they are not being implemented in this specific class. Similarly in the other two boards the same thing happens with their own set of sensors.

This means that in the DiWheelDrive class, the functions that are being implemented correspond to the gyroscope, accelerometer, magnetometer, proximity sensors, motors, and encoders. In the Power Management class the implemented functions include proximity sensors and power status sensors, and in the Light Ring class the implemented function concerns only the LEDs for the purposes of DA\_AMiRo.

The global class is the responsible for creating each object from each sensor and encapsulating them under the name of 'robot'. This makes the \textit{robot's} attributes accessible from other classes. This means that, for example, the gyroscope's value can be updated and outputted as a part of \textit{robot} from other parts of the code. The global object is finally initialized in the main function, as well as the threads responsible for managing the scheduler of the sensor's data but this subject will be discussed more deeply in chapter \ref{chap:sensorDH}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{DWDclass}
    \caption{DiWheelDrive broad class diagram}
    \label{fig:DWDclass}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{PMclass}
    \caption{PowerManagement broad class diagram}
    \label{fig:PMclass}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{LRclass}
    \caption{LightRing broad class diagram}
    \label{fig:LRclass}
\end{figure}

\chapter{Controller Area Network}
\label{chap:CAN}
This chapter explains how to receive and transmit CAN frames in the AMiRo. The functionality of the CAN module had to be merged with the current ControllerAreaNetworkRx.cpp and ControllerAreaNetworkTx.cpp of the existing repository of AMiRo \cite{AMiRo_Wiki}. The ControllerAreaNetwork.h however, is the same that the DAEbot project is using. The reason behind sharing the ControllerAreaNetwork.h among projects is to be able to use a single Relfective Operator for both systems AMiRo and DAEbot so it is easier to compare functionalities.

For starters, the baud rate requirement of the DAEbot project was set to 500kHz, this is why the same frequency was selected for the DA\_AMiRo. This frequency was achieved with the following equation, and can be changed from the CANRx and CANtx.cpp files that can be located with help of figure \ref{fig:OS}.

\begin{equation} \label{eq:baudrate1}
	500kHz= \frac{8MHz}{(B1+B2+SW)*prescaler}
\end{equation}

Where the 8MHz is the clock frequency of the STM32F3, the B1 and B2 are the time quanta bit segment one and two accordingly. Finally, the SW represents the maximum time quanta. These values were configured by the previous AMiRo project which also happened to configure the CAN frequency to 500kHz. The values and the names of the variables in the actual code are: $$B1 = CAN\_BTR\_TS1 = 13$$ $$B2 = CAN\_BTR\_TS2 = 2$$ and $$SW = CAN\_BTR\_SJW = 1$$ $$Prescaler = CAN\_BTR\_BRP = 1$$, which results in the following equation:

\begin{equation} \label{eq:baudrate2}
	\frac{8MHz}{(13+2+1)*1} = 500kHz
\end{equation}

\section{Convention used for sending and receiving frames}
\label{sec:canconv}

The format of the CAN frames was done accordingly with the pre-established convention for the DAEbot project \cite{DAEbot_Wiki}. The format of the standard id from a CAN frame can be seen in figure \ref{fig:can}, where it is basically divided into three main parts. The command or sensor bit is the bit that allows devices on the CAN bus to know if the frame belongs to a command, i.e. knowing the value 'x' axis of the gyroscope, or setting the LEDs to red, or if it belongs to a sensor frame, i.e. the actual value of the 'x' axis of the gyroscope.

The second part is the priority which allows the devices on the bus know the priority of the command or sensor. If there are two frames in the bus at the same time asking for a sensor value from the same board for example, the priority will allow the system to preempt the frame of the one with higher priority.

The third and last part of the id from the frame is the topic id. The list for the topics can be seen in the ControllerAreaNetwork.h shared by all the devices or also from the ods file inside the same path \textit{Peripheral-Drivers/ControllerAreaNetwork/} from figure \ref{fig:DAAMIROstr}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{can_structure}
    \caption{Pre-established CAN frame format \cite{DAEbot_Wiki}}
    \label{fig:can}
\end{figure}

For the transmission of a CAN frame, the id is coded according to figure \ref{fig:can} and when a CAN frame is received, the id is decoded into the same structure. The CAN frame pre-established structure can be seen in snip of code \ref{CAN:frame}, which can be compared with the figure \ref{fig:can}. In this structure the only thing that has not been yet discussed is the data itself. The dlc is the number of bytes to be transmitted and the information is stored in the data8, data16 and data32 arrays. Thanks to the \textit{union} of these arrays, the same value can be accessed in different bit sizes.

\begin{minipage}{\linewidth}
\begin{lstlisting}[caption = CAN frame, label = CAN:frame, language = C, captionpos = b]
typedef struct can_frame_types {
    priority_can_id_t priority_id;
    c_s_bit_can_id_t c_s_bit_id;
    topic_can_id_t topic_id;
    uint8_t dlc;
    union{
        uint8_t     data8[8];
        uint16_t    data16[4];
        uint32_t    data32[2];
    };
} can_frame_types_t;
\end{lstlisting}
\end{minipage}

In order to send a CAN frame, can\_transmit\_data\_frame\_pd(...) can be called. When sending a CAN frame, the id is set to be in standard mode. The CAN frame that is sent consists of three main parts: id, DLC, and the data. The id is therefore set with the can\_code\_identifier\_pd(...) function before actually calling the transmit function. This function combines the c\_s bit with the priority bit and then combines that value to the topic ID. The CAN frame is later sent through the CAN driver.

On the CAN reception side, the status of the reception is assigned when calling to the can\_receive\_data\_frame\_pd(...) function. The status of the CAN reception can be successful, timeout, or error. Then the id of the CAN frame gets decoded with the function can\_decode\_identifier\_pd(...) and the data frames are passed into a global structure like in snip of code \ref{CAN:frame}

Note that can\_code\_identifier\_pd(...) and can\_decode\_identifier\_pd(...) are functions that were already implemented beforehand, for this project it was necessary to adjust both functions to operate with the current hardware platform.

%\clearpage
\section{Interpreting CAN frames of the sensors}
\label{sec:sensorout}
In this section it will be explained how to read the frames corresponding to each sensor. For the same reason, this section is divided into two subsections: sensors and actuators.

\subsection{Sensors}
\label{sub:sensors}
In order to interact with a sensor, the id from the CAN frame's command has to obey the convention shown in figure \ref{fig:can} and its content, the actual data from the CAN frame has to follow the convention shown in figure  \ref{fig:sensormodes} created by Uwe Jahn for the DAEbot project.

There are three ways or modes in which a command can ask for data from a sensor. The first one is the mode one which asks for a sensor's data with a specific frequency. The first DLC of the CAN frame has to be set to '1' to activate the publisher mode. The second and third DLCs specify the numeric value of the frequency in format of little-endian. This means that the most significant bit should be a part of the DLC number 2 and the less significant bit should be a part of the DLC number 1. The fourth DLC specify the units of the value given in the second and third DLCs. This value can be obtained looking at figure \ref{fig:sensormodes}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{modes}
    \caption{Pre-established CAN modes for transmitting sensor data \cite{DAEbot_Wiki}}
    \label{fig:sensormodes}
\end{figure}

For example for starting publisher of the magnetometer 'x' axis, with normal priority, the CAN id should be composed by:
\begin{itemize}
	\item C\_S bit set to '0' on binary
	\item Priority set to '10' on binary
	\item Topic id of the gyroscope 'x' axis '0x05' on hexadecimal
\end{itemize}

So a value equal of '01000000101' in binary or '0x205' in hexadecimal. Now lets say we want the magnetometer 'x' axis value every $900\mu s$. The value '900' in decimal is '0x0384' in hexadecimal, making the little-endian adjustment for the bytes will result in '8403'. $\mu s$ is equal to '0x02'.

As a result, the complete CAN frame is: '0x205' for the id, '4' for the DLC or Data Length Code, each with the following values:

\begin{itemize}
	\item DLC 0: 0x01
	\item DLC 1: 0x84
	\item DLC 2: 0x03
	\item DLC 3: 0x02
\end{itemize}

This command would trigger the DiWheelDrive to output the magnetometer 'x' axis value every $900\mu s$, with an id that is almost the same but with the C\_S bit set to '1' instead, so that the CAN bus knows that this frame belongs to a sensor data. The topic id then will be '0x605' instead of '0x205'. The way of reading and understanding the outputted value for each sensor will be mentioned in the following subsections.

The second mode is the \textit{One-time data transmission} and it will only return the required sensor value one time. For the example of a one time transmission of the floor proximity sensors, the id of the CAN frame should be '0x204' with DLC 0: 2.

The third mode is the mode '0' which actually stops an on-going publishing of a sensor value given the frame's id. This means that in the example used for the explanation of mode '1' the way of stopping the transmission of the magnetometer 'x' axis, the id of the CAN frame should be the same '0x205', with DLC 0: 0.

\subsubsection{Gyroscope}
The three axis of the gyroscope come together in one CAN frame with a DLC of six. Each value has a size of 16bits with a format  of little-endian. Every value comes in two's complement and the units are degrees per second, and the resolution is $+/-500 dps$. See table \ref{tab:gyroscope}.

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|}
	\hline
	DLC						&	6													\\	\hline
	size of data	&	16bits										\\	\hline
	'x' axis			&	DLC[1] DLC[0]							\\	\hline
	'y'	axis			&	DLC[3] DLC[2]							\\	\hline
	'z'	axis			&	DLC[5] DLC[4]							\\	\hline
	units					&	dps												\\	\hline
	note					& value in two's complement \\	\hline
\end{tabular}
\caption{\label{tab:gyroscope} CAN frame format from the output of the gyroscope}
\end{table}

\subsubsection{Accelerometer}
The three axis of the accelerometer come together in one CAN frame with a DLC of six as the gyroscope. Each value has a size of 16bits with a format of little-endian. Every value comes in two's complement and the units are $g$, and the result is scaled $+/-8g$. See table \ref{tab:accelerometer}.

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|}
	\hline
	DLC						&	6													\\	\hline
	size of data	&	16bits										\\	\hline
	'x' axis			&	DLC[1] DLC[0]							\\	\hline
	'y'	axis			&	DLC[3] DLC[2]							\\	\hline
	'z'	axis			&	DLC[5] DLC[4]							\\	\hline
	units					&	g													\\	\hline
	note					& value in two's complement \\	\hline
\end{tabular}
\caption{\label{tab:accelerometer} CAN frame format from the output of the accelerometer}
\end{table}

\subsubsection{Magnetometer}
In the case of the Magnetometer, every axis has an independent id, this is because the size of the data is 32bits in little-endian format. Every value comes in two's complement and the units are $Gauss$, and the result is scaled by $+/-5Gauss$. See table \ref{tab:magnetometer}.

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|}
	\hline
	DLC						&	4														\\	\hline
	size of data	&	32bits											\\	\hline
	'x' axis			&	DLC[3] DLC[2] DLC[1] DLC[0]	\\	\hline
	'y'	axis			&	DLC[3] DLC[2] DLC[1] DLC[0]	\\	\hline
	'z'	axis			&	DLC[3] DLC[2] DLC[1] DLC[0]	\\	\hline
	units					&	Gauss												\\	\hline
	note					& value in two's complement 	\\	\hline
\end{tabular}
\caption{\label{tab:magnetometer} CAN frame format from the output of the magnetometer}
\end{table}

\subsubsection{Floor proximity sensors}
The four proximity sensors on the bottom of the AMiRo are together in one CAN frame with a DLC of eight. Each value has a size of 16bits with a format of little-endian. The data is in luminance with units candelas per squared meter $cd/m^2$. See table \ref{tab:floorprox} and note that this sensor does not return a two's complement value.

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|}
	\hline
	DLC						&	8													\\	\hline
	size of data	&	16bits										\\	\hline
	sensor 1			&	DLC[1] DLC[0]							\\	\hline
  sensor 2			&	DLC[3] DLC[2]							\\	\hline
	sensor 3			&	DLC[5] DLC[4]							\\	\hline
	sensor 4			&	DLC[7] DLC[6]							\\	\hline
	units					&	$cd/m^2$									\\	\hline
\end{tabular}
\caption{\label{tab:floorprox} CAN frame format from the output of the floor proximity sensors}
\end{table}

\subsubsection{Encoder}
This sensor returns both the actual velocity and the odometry of the AMiRo. The first value will be the actual velocity followed by the odometry. For the odometry, the 'x' coordinate in $\mu m$ with a size of 32bits, the 'y' coordinate in $\mu m$ with a size of 32bits, and the orientation of the wheels in $\mu rad$ with a size of 16bits. All of these values are together in one CAN frame. See table \ref{tab:encoder}.

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|}
	\hline
	DLC										&	8																\\	\hline
	size of data					&	32 bits, 32 bits, and 16 bits		\\	\hline
	'x' coordinate				&	DLC[0] DLC[1] DLC[2] 						\\	\hline
	'y'	coordinate				&	DLC[3] DLC[4] DLC[5] 						\\	\hline
	orientation of wheels	&	DLC[6] DLC[7] 									\\	\hline
	units									&	$\mu m$, $\mu m$, and $\mu rad$	\\	\hline
\end{tabular}
\caption{\label{tab:encoder} CAN frame format from the encoder}
\end{table}

\subsubsection{Actual velocity}
The actual velocity will be composed of the 'x' velocity in $\mu m/s$ with a size of 32bits and the 'z' angular velocity in $\mu rad/s$ also in 32bits. These two values are together in one CAN frame. See table \ref{tab:velocity}.

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|}
	\hline
	DLC										&	8														\\	\hline
	size of data					&	32bits											\\	\hline
	'x' velocity					&	DLC[0] DLC[1] DLC[2] DLC[3]	\\	\hline
	'z'	angular velocity	&	DLC[4] DLC[5] DLC[6] DLC[7]	\\	\hline
	units									&	$\mu m/s$ and $\mu rad/s$		\\	\hline
\end{tabular}
\caption{\label{tab:velocity} CAN frame format from the output of the actual velocity}
\end{table}

\subsubsection{Power Status}
When the id for the power status is asked, four values are sent within one CAN frame. The charging or no charging flag in 8bits, the state of charge in percentage in 8bits, the minutes remaining for a complete charge when the flag for charging is 'on' and the minutes remaining of battery power when the flag for charging is 'off' in 16bits and in little-endian, and the current power consumption in $mW$ also in 16 bits and in little-endian. See table \ref{tab:powerstatus}.

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|}
	\hline
	DLC										&	6																	&																					\\	\hline
	size of data					&	8bits, 8bits, 16bits, and 16bits	&																					\\	\hline
	charging flag					&	DLC[0]														& '0' for not charging, '1' for charging	\\	\hline
	state of charge				&	DLC[1] 														&	percentage															\\	\hline
	time until charge			&	DLC[3] DLC[2] 										&																					\\	\hline
	power consumption			&	DLC[5] DLC[4] 										&																					\\	\hline
	units									&	percentage, minutes, $mW$					&																					\\	\hline
\end{tabular}
\caption{\label{tab:powerstatus} CAN frame format from the output of the power status}
\end{table}

\subsubsection{Proximity ring}
The proximity ring will return eight CAN frames one for every sensor there is. Each value is of 16btis and it is in little-endian, and the units are $mm$. See table \ref{tab:proxring}.

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|}
	\hline
	DLC						&	16												\\	\hline
	size of data	&	16bits										\\	\hline
	sensor 1			&	DLC[1] DLC[0]							\\	\hline
  sensor 2			&	DLC[1] DLC[0]							\\	\hline
	sensor 3			&	DLC[1] DLC[0]							\\	\hline
	sensor 4			&	DLC[1] DLC[0]							\\	\hline
	sensor 5			&	DLC[1] DLC[0]							\\	\hline
  sensor 6			&	DLC[1] DLC[0]							\\	\hline
	sensor 7			&	DLC[1] DLC[0]							\\	\hline
	sensor 8			&	DLC[1] DLC[0]							\\	\hline
	units					&	$mm$											\\	\hline
\end{tabular}
\caption{\label{tab:proxring} CAN frame format from the output of the proximity ring sensors}
\end{table}

\subsection{Actuators}
There are so far two implemented actuators: the motors and the LEDs. The way of interacting with the sensors will be now described but it is important to note that they will not send anything back to the CAN bus, as they will only 'act'.

\subsubsection{Motors}
The motors can be started with a desired speed in $\mu m/s$ or in an angular speed with $\mu rad/s$ units. See table \ref{tab:setvelocity}. For the id of the motor's actuator please refer to the ControllerAreaNetwork.h file found in \ref{fig:DAAMIROstr}.

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|}
	\hline
	DLC										&	8														\\	\hline
	size of data					&	32bits											\\	\hline
	'x' velocity					&	DLC[0] DLC[1] DLC[2] DLC[3]	\\	\hline
	'z'	angular velocity	&	DLC[4] DLC[5] DLC[6] DLC[7]	\\	\hline
	units									&	$\mu m/s$ and $\mu rad/s$		\\	\hline
\end{tabular}
\caption{\label{tab:setvelocity} CAN frame format for setting the velocity of the motors}
\end{table}

\subsubsection{LEDs}
\label{sub:CANleds}
In order to turn a LED on, the following data structure has to be followed:

\begin{itemize}
	\item id: Desired LED's id obtainable from the ControllerAreaNetwork.h found in \ref{fig:DAAMIROstr}.
	\item DLC 0: Red value
	\item DLC 1: Green value
	\item DLC 2: Blue value
\end{itemize}

\chapter{Sensor data handler}
\label{chap:sensorDH}
The sensor data handler is the responsible for setting the publisher of sensor's data. The publisher can handle every sensor sending its value at once as they have each a dedicated task in a real-time context. This means that every sensor has its own task assigned to it and said task gets suspended or resumed with the requested frequency according to the commands of the CAN frames.

In figure \ref{fig:sensorclass} the structure of the sensor data handler can be seen as well as its connection with the sensor tasks. All of these tasks get managed by the sensor data instance.

\begin{figure}[ht]
	\includegraphics[width=15cm]{sensorclass}% 20cm, height = 10cm, angle = 90]{sensorclass}
  	\caption{SensorDataHandler.cpp and the sensor tasks structure}
  	\label{fig:sensorclass}
\end{figure}

\section{Overall functionality}
The sensor handler is a class which has only once instance and its started in the main class, along with the tasks for all the sensors inside each board. When the task for the sensor handler is started, it polls the CAN reception for a CAN frame that has the C\_S bit set to a '0' which corresponds to a \textit{Command data}, according to fig \ref{fig:can} of section \ref{sec:canconv}.

Whenever the sensor handler detects a CAN frame corresponding to a \textit{Command data}, the CAN frame has to be further analyzed to know if it is trying to communicate with a \textit{sensor} or with an \textit{actuator}. An example of this functionality can be seen in figure \ref{fig:seqoverall} which will be explained in section \ref{sub:rtimp}.

\subsection{Sensor data handling}
Whenever the command corresponds to a sensor, the next step is to read the DLC 0 to know if it is a command for stopping a sensor data, if it is a command for starting a publishing sensor data, or if it is a request for a one time transmission, see figure \ref{fig:sensormodes} from section \ref{sub:sensors}.

The state machine of figure \ref{fig:states} shows how the modes get handled. Whenever the DLC 0 has a '0' corresponding to a \textit{No transmission} is detected, the id from the topic gets identified and the corresponding task gets suspended so it no longer transmits its value.

When the DLC 0 has a '1' corresponding to a \textit{Publisher} is detected, the DLC 1, DLC 2, and DLC 3 get decoded to know the requested frequency from the id of the sensor that needs to start sending its information. If the sensor was already sending information, it needs to update its frequency and the priority, and it gets going.

Finally, when the DLC 0 has a '2' which means that the user wants a \textit{One time transmission} of a sensor data, this value gets sent one time only. If this sensor was sending its value in a publisher, then the task gets suspended.

\begin{figure}[ht]
 \centering
 \includegraphics[width=\textwidth]{states}
		\caption{Modes implementation}
		\label{fig:states}
\end{figure}

\subsection{Actuator data handling}
In the other case, when the command is trying to communicate with an actuator, it goes into a sorter that separates the motor's command from the LEDs' command. If it is a command for the motors, then the motors get set with the velocity requested in the CAN frame. When its a LEDs command, the selected LED gets turn on or changes its color with the \textit{RGB} code reviewed in section \ref{sub:CANleds}.

\subsection{Real-time implementation}
\label{sub:rtimp}
The tasks corresponding to the sensors can all be operating at once. The priority of the tasks is being handled as preemptive tasks. This means that if two tasks are racing for the processing time, the one with higher priority will be preempted. Also, if one task is asked to output its value at the same frequency as a task that is already outputting its value, the one with higher priority will be on schedule and the one with less priority can be delayed some $\mu s$.

Everything up to this point works as an independent sensor data handler. This means that the internal controller is encapsulated and complete. The only thing that can interact with the functioning system are commands in the CAN bus. For this purpose the MuRox board has to be set and running the Linux based Operating System. See next chapter for more information about how to debug and the functionality of the MuRoX board in general.

An example of how the controller and the MuRoX board are communicating can be seen in figure \ref{fig:seqoverall}. The three boards are constantly checking the CAN bus for a CAN id corresponding to a \textit{command}. Whenever the MuRoX board sends a command asking for the gyroscope value with a specific frequency, this CAN frame gets fetched by the \textit{DiWheelDrive} board and starts the thread of the gyroscope with the requested frequency and priority.

In the image the MuRoX board asks then for a proximity ring value. This sensor is located on the \textit{Power Management} board so this thread gets started by this board. Afterwards, the MuRoX board asks to set the motor with a specific speed. The \textit{DiWheelDrive} board where the motors are connected, fetches this information and sets the motors accordingly.

The same process gets carried out for the odometry value request, which is managed also by the \textit{DiWheelDrive} board. The MuRoX board asks to stop the gyroscope sensor from sending its value. This command also gets fetched by the same board and puts the corresponding thread to sleep.

Finally the MuRoX board asks for the LED number 3 to turn on with a specific RGB value. This information is fetched by the \textit{Light Ring} board and turns on the LED with the requested information.

\begin{figure}[ht]
 \centering
 \caption{Functionality of an example application}
 \includegraphics[width=\textwidth, height=18cm]{sequence_overall}
		\label{fig:seqoverall}
\end{figure}

\chapter{MuRoX board}
\label{chap:reflective}
This board is performing as the \textit{Reflective Operator} of the system. This board is the one that asks for a certain sensor data from the internal controller or also it can control the actuators. The user can connect to this board through ssh, with the address 192.168.1.1 with password \textit{root}.

The board already has a package called 'candump' which interacts with the CAN module in the board and can be used to debug the internal controller's functionality. The way of sending CAN frames from the MuRoX board has to follow the next convention:

\begin{lstlisting}[caption = CAN send format, label = CAN:debug, captionpos = b]
cansend can0 --identifier=CANid DLC0 DLC1 DLC2 DLC3 DLC4 ...
\end{lstlisting}

\section{Compiling for MuRoX}
\label{sec:poky}
For creating an application that the MuRoX board can run, it is necessary to use the \textit{cortexa8hf-vfp-neon-poky-linux-gnueabi} version of the cross-compiler. Once the application is compiled, the resulting binaries can be moved into the board using the \textit{scp} command.

\begin{lstlisting}[caption = moving files into MuRoX board \cite{AMiRo_Murox}, label = CAN:debug, captionpos = b]
scp anyFile root@192.168.0.1:~
\end{lstlisting}

\section{Compiling from a Simulink model for MuRoX}
It is intended for a Simulink model to be compiled and programmed into the MuRoX board. For this purpose, the Simulink model gets translated into C code with the help of Matlab's solver. The resulting code has to be passed through the process described in section \ref{sec:poky} so it is compatible with the MuRoX board.

It is worth mentioning that the Simulink model is being done in cooperation but mostly by Uwe Jahn and it is still work in progress.

\section{Example applications}
\label{sect:example}
For testing purposes, three demo applications were created. These three applications are already inside the MuRoX board, but the user can modify the source code which is obtainable from the DA\_AMiRo git repository \cite{AMiRo_Git}. The modified versions of the source code can be compiled and uploaded to the MuRoX board using the method already described in section \ref{sec:poky}. The three demos are:

\begin{itemize}
  \item \textit{Demo Sender Reflective Operator}
  \item \textit{Demo Receiver Reflective Operator}
	\item \textit{Demo Standalone Reflective Operator}
\end{itemize}

The first one is called \textit{Demo Sender Reflective Operator} and it is a simple application where the user can send commands to the internal controller with assistance of a set of menus. The application asks for the priority, the topic id, the mode to be set, and all the needed fields for each case. It also separates actuators from sensors. This application facilitates the sending of commands, because the user does not have to memorize the topic ids or the CAN frame conventions.

In figures \ref{fig:sender1} and \ref{fig:sender2} a possible usage of the demo is shown. The menus can be seen, as the first one asks if the user wants to talk with an actuator or a sensor. If the answer is an actuator, like in figure \ref{fig:sender2} the second menu is to select which actuator to use and finally to set the according values to the desired actuator. If the user wants to talk with a sensor like in figure \ref{fig:sender1}, the menus ask for the mode: stop, publisher, and one time transmission. Then it proceeds to ask the frequency if it was publisher, the topic and the priority.

\begin{figure}[ht]
 \centering
 \includegraphics[width=\textwidth, height=18cm]{sender_example}
		\caption{Example of a \textit{Demo Sender Reflective Operator} usage}
		\label{fig:sender1}
\end{figure}

\begin{figure}[h]
 \centering
 \includegraphics[width=\textwidth, height=8cm]{sender_example2}
		\caption{Example of a \textit{Demo Sender Reflective Operator} usage, part 2}
		\label{fig:sender2}
\end{figure}

The second application is called \textit{Demo Receiver Reflective Operator} and it is basically a thread that outputs the information that the CAN bus receives corresponding to the sensors. The information is translated in English so the user can read it easily without the necessity of the convention shown in figures \ref{fig:can} and \ref{fig:sensormodes}. The output shows the priority, a time stamp upon the reception of the CAN frame, the topic of the sensor's data, the value, and its units. This can be seen in figure \ref{fig:receiver1}, where an example of how the receiver displays the information of some of the sensors.

\begin{figure}[ht]
 \centering
 \includegraphics[width=\textwidth, height=18cm]{receiver_example}
		\caption{Example of a \textit{Demo Receiver Reflective Operator} usage}
		\label{fig:receiver1}
\end{figure}

The idea behind these two applications \textit{Demo Sender Reflective Operator} and \textit{Demo Receiver Reflective Operator} is that the user runs both at the same time in two different terminal windows to send commands and to see how they are being answered in real time.

The third application is called \textit{Demo Standalone Reflective Operator}. It has a determined functionality so the user needs only to run it in order to see its results with the help of the \textit{Demo Standalone Reflective Operator} or the \textit{candump} package. This Demo has a set of commands to be sent to the internal controller and shows the outputs of the sensors. The functionality will be

%explicar el standalone
%poner imágenes de cada applicación demo...

Lastly, the user can also send independent CAN frames using the \textit{candump} package installed in the MuRoX board, as it was already discussed in chapter \ref{chap:reflective}. For this method, the user has to decode previously the topic id and the CAN data frames according to images \ref{fig:can} and \ref{fig:sensormodes}.

\chapter{Results}
In this chapter the structure and functionality of both projects DA\_AMiRo and DAEbot are compared. As it was intended, this project will help to improve the behavior, structure, and functionality of the DAEbot. Through a brief analysis, some recommendations for the DAEbot project are made for future work.

\section{AMiRo vs. DAEbot}
%Aquí van los resultados, imágenes de resultados y explicación.


In a quick glance at the results, the DA\_AMiRo seems to be slightly better in functionality. The results in DAEbot have a 100ns - 1ms timeouts inaccuracy which compared to the approximate $545\mu s$ timeouts for the DA\_AMiRo are for starters a big disadvantage for a real-time application.

In both systems, the multi threading gets compromised and looses some of its reliability because the CAN module is just one and it needs approximately $545\mu s$ to send one frame. This was obtained with the following equation: $$1 us   (1+11+1+1+1+4+64+15+1+1+1+7)*5= 545 us$$ $$1/(1MHz)*(SOF+ID+RTR+IDE+RES+DLC+DATA+CRC+$$ $$DELIM+ACK+DELIM+EOF)*No. RETRIES$$
taken from the original code of the repository \cite{AMiRo_Wiki}.

\section{Outlook}
The DAEbot software structure is currently functioning with a fixed number of tasks which is less than the total amount of sensors. This means that the DAEbot can't send every sensors' data at once. Even though this issue is being fixed and it appears the DAEbot will be able to send every sensors' data at once, as the present work is being done, this issue has not been fixed yet.

On top of the software structure, DA\_AMiRo has three STM boards which allows more computational power compared to the one STM board destined to the DAEbot project. This advantage of the DA\_AMiRo's computational power makes it able to handle more parallel tasks than the DAEbot for starters.

Now lets look at advantages of the DAEbot. The DA\_AmiRo's computational power is too big regarding the current application, this means that there is a lot of computational power that the project is not using. The DAEbot was build for this specific application which means that the resources can be focused on it without being wasted. Even though this does not seem like a big deal, it is a huge advantage as all its resources are being used with a common goal and there is no computational power being used in other parts of the project that does not contribute to the main objective.

A good option for improving the DAEbot's functionality would be to replace the current system of the fixed number of tasks being handled by an array of tasks, with an approach like the one of the DA\_AMiRo, where every sensor has its own task which can be handled completely by the sensor data handler.


%sequence diagram (varios)

%bibliography
\printbibliography
\nocite{*}

\end{document}
\grid
